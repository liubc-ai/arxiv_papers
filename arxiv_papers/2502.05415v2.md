# UniCMs: A Unified Consistency Model For Efficient Multimodal Generation
  and Understanding

**作者**: Chenkai Xu, Xu Wang, Zhenyi Liao, Yishun Li, Tianqi Hou, Zhijie Deng

**发表时间**: 2025-02-08T02:52:25Z

**更新时间**: 2025-05-18T14:59:21Z

**PDF链接**: [http://arxiv.org/pdf/2502.05415v2](http://arxiv.org/pdf/2502.05415v2)

**arXiv ID**: 2502.05415v2

## 摘要

Consistency models (CMs) have shown promise in the efficient generation of
both image and text. This raises the natural question of whether we can learn a
unified CM for efficient multimodal generation (e.g., text-to-image) and
understanding (e.g., image-to-text). Intuitively, such a model could be
acquired by applying the consistency distillation (CD) to existing unified
multimodal models. However, the key challenge is establishing a unified
denoising perspective for both image and text generation, which is essential
for establishing the consistency mapping. To tackle this, at the representation
level, we advocate for discrete tokens for both modalities to best preserve
language modeling capabilities. Critically, instead of defining the text
denoising trajectory via recent discrete diffusion language modeling
principles, we specify it using the parallel decoding trace of an
autoregressive language model, benefiting from the latter's superior
performance in general text generation tasks. The denoising trajectory of image
tokens adheres to standard discrete diffusion. We train our unified consistency
models (UniCMs) on these combined multimodal trajectories simultaneously with a
unified objective. We introduce a trajectory segmentation strategy to further
improve the training convergence. Empirically, in text-to-image generation,
UniCMs outperform SD3 on GenEval, Image Reward, and CLIP Score metrics, while
requiring only approximately ${1}/{8}$ of the sampling time. Meanwhile, in
image-to-text generation, UniCMs surpass Show-o on the MMMU benchmark while
being $1.5 \times$ faster at long-sequence generating speed. The code is
available at https://github.com/zhijie-group/UniCMs.
