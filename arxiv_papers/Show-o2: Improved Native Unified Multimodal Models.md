# Show-o2: Improved Native Unified Multimodal Models

**作者**: Jinheng Xie, Zhenheng Yang, Mike Zheng Shou

**发表时间**: 2025-06-18T15:39:15Z

**更新时间**: 2025-06-20T08:39:17Z

**PDF链接**: [http://arxiv.org/pdf/2506.15564v2](http://arxiv.org/pdf/2506.15564v2)

**arXiv ID**: 2506.15564v2

## 摘要

This paper presents improved native unified multimodal models, \emph{i.e.,}
Show-o2, that leverage autoregressive modeling and flow matching. Built upon a
3D causal variational autoencoder space, unified visual representations are
constructed through a dual-path of spatial (-temporal) fusion, enabling
scalability across image and video modalities while ensuring effective
multimodal understanding and generation. Based on a language model,
autoregressive modeling and flow matching are natively applied to the language
head and flow head, respectively, to facilitate text token prediction and
image/video generation. A two-stage training recipe is designed to effectively
learn and scale to larger models. The resulting Show-o2 models demonstrate
versatility in handling a wide range of multimodal understanding and generation
tasks across diverse modalities, including text, images, and videos. Code and
models are released at https://github.com/showlab/Show-o.

## 内容总结

无法生成总结: PDF内容或图片提取失败

## 方法总览图

