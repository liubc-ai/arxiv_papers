# Omni-Video: Democratizing Unified Video Understanding and Generation

**作者**: Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, Hao Li

**发表时间**: 2025-07-08T16:02:16Z

**更新时间**: 2025-07-09T12:27:27Z

**PDF链接**: [http://arxiv.org/pdf/2507.06119v2](http://arxiv.org/pdf/2507.06119v2)

**arXiv ID**: 2507.06119v2

## 摘要

Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.
