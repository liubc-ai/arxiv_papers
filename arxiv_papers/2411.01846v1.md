# KptLLM: Unveiling the Power of Large Language Model for Keypoint
  Comprehension

**作者**: Jie Yang, Wang Zeng, Sheng Jin, Lumin Xu, Wentao Liu, Chen Qian, Ruimao Zhang

**发表时间**: 2024-11-04T06:42:24Z

**更新时间**: 2024-11-04T06:42:24Z

**PDF链接**: [http://arxiv.org/pdf/2411.01846v1](http://arxiv.org/pdf/2411.01846v1)

**arXiv ID**: 2411.01846v1

## 摘要

Recent advancements in Multimodal Large Language Models (MLLMs) have greatly
improved their abilities in image understanding. However, these models often
struggle with grasping pixel-level semantic details, e.g., the keypoints of an
object. To bridge this gap, we introduce the novel challenge of Semantic
Keypoint Comprehension, which aims to comprehend keypoints across different
task scenarios, including keypoint semantic understanding, visual prompt-based
keypoint detection, and textual prompt-based keypoint detection. Moreover, we
introduce KptLLM, a unified multimodal model that utilizes an
identify-then-detect strategy to effectively address these challenges. KptLLM
underscores the initial discernment of semantics in keypoints, followed by the
precise determination of their positions through a chain-of-thought process.
With several carefully designed modules, KptLLM adeptly handles various
modality inputs, facilitating the interpretation of both semantic contents and
keypoint locations. Our extensive experiments demonstrate KptLLM's superiority
in various keypoint detection benchmarks and its unique semantic capabilities
in interpreting keypoints.
