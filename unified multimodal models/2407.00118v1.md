# From Efficient Multimodal Models to World Models: A Survey

**作者**: Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang

**发表时间**: 2024-06-27T15:36:43Z

**更新时间**: 2024-06-27T15:36:43Z

**PDF链接**: [http://arxiv.org/pdf/2407.00118v1](http://arxiv.org/pdf/2407.00118v1)

**arXiv ID**: 2407.00118v1

## 摘要

Multimodal Large Models (MLMs) are becoming a significant research focus,
combining powerful large language models with multimodal learning to perform
complex tasks across different data modalities. This review explores the latest
developments and challenges in MLMs, emphasizing their potential in achieving
artificial general intelligence and as a pathway to world models. We provide an
overview of key techniques such as Multimodal Chain of Thought (M-COT),
Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning
(M-ICL). Additionally, we discuss both the fundamental and specific
technologies of multimodal models, highlighting their applications,
input/output modalities, and design characteristics. Despite significant
advancements, the development of a unified multimodal model remains elusive. We
discuss the integration of 3D generation and embodied intelligence to enhance
world simulation capabilities and propose incorporating external rule systems
for improved reasoning and decision-making. Finally, we outline future research
directions to address these challenges and advance the field.
